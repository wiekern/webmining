{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment1 Web Mining \n",
    "## Ruikun Li 382591; Yafei Yan 382560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import requests, json, os, math, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def listdir_nohidden(path):\n",
    "    for f in os.listdir(path):\n",
    "        if not f.startswith('.'):\n",
    "            yield f\n",
    "        \n",
    "def remove_stopwords(words):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    return [ word for word in words if word not in stop and word.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling webpages & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to doc [titles=Wikipedia:Manual of Style/Dates and numbers&] failed.\n",
      "Writing to doc [titles=Portal:Contents/Portals&] failed.\n"
     ]
    }
   ],
   "source": [
    "# Extracting desired webpages\n",
    "debug = 1\n",
    "extract_flag = True\n",
    "base_url = 'https://en.wikipedia.org/w/api.php?'\n",
    "action = 'action=query&'\n",
    "title = 'titles=Web%20design&'\n",
    "prop = 'prop=links&format=json&pllimit=max'\n",
    "target_dir = './docs/'\n",
    "\n",
    "if extract_flag:\n",
    "    results = requests.get(base_url + action + title + prop)\n",
    "    contents = json.loads(results._content)\n",
    "    links_res = contents['query']['pages']['34035']['links']\n",
    "    links_res.append({'title':'Web design'})\n",
    "    for link in links_res:\n",
    "        title_in_page = link['title']\n",
    "        title = 'titles=' + title_in_page + '&'\n",
    "        #prop = 'format=json&prop=revisions&rvprop=content&rvsection=0'\n",
    "        prop = 'format=json&prop=extracts&exintro=&explaintext='\n",
    "        results = requests.get(base_url + action + title + prop)\n",
    "        contents = json.loads(results._content)\n",
    "        pages = contents['query']['pages']\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "        for key in pages:\n",
    "            try:\n",
    "                content = pages[key]['extract']\n",
    "                if os.path.isfile(target_dir + title_in_page + '.txt'):\n",
    "                    pass\n",
    "                else:\n",
    "                    with open(target_dir + title_in_page + '.txt', \"w+\") as file:\n",
    "                        tokens = remove_stopwords(word_tokenize(content.lower()))\n",
    "                        stemmer = SnowballStemmer(\"english\")\n",
    "                        s = [stemmer.stem(w) for w in tokens]\n",
    "                        file.write(u' '.join(s))\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IOError:\n",
    "                print('Writing to doc [' + title +'] failed.')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize with nltk, removing stopwords\n",
    "distinct_terms = {}\n",
    "terms_by_id = {}\n",
    "doc_names = []\n",
    "doc_tokenized = []\n",
    "number_docs = 0\n",
    "doc_lens = []   # for statistical model\n",
    "for fs in os.listdir(target_dir):\n",
    "    doc_names.append(fs)\n",
    "    with open(target_dir + fs, 'r') as f:\n",
    "        read_data = f.read()\n",
    "        if read_data:\n",
    "            number_docs += 1\n",
    "            s = word_tokenize(read_data)\n",
    "            doc_lens.append(len(s))\n",
    "            doc_tokenized.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing auxiliary data structures which is applied at calculating TFIDF, Probabilities\n",
    "def tf_df_maxf(docs):\n",
    "    terms = []\n",
    "    max_freqs = []\n",
    "    term_id = 0\n",
    "    for i in range(len(docs)):\n",
    "        new_doc = True\n",
    "        considered = True\n",
    "        max_freq = 0\n",
    "        terms.append({})\n",
    "        for term in docs[i]:\n",
    "            if distinct_terms.get(term, -1) == -1:\n",
    "                distinct_terms[term] = term_id\n",
    "                terms_by_id[term_id] = term\n",
    "                term_id += 1\n",
    "            if not terms[i]:\n",
    "                terms[i] = {term: [1, 1]}\n",
    "            elif terms[i].get(term, 'NA') == 'NA':\n",
    "                terms[i][term] = [1, 1]\n",
    "            else:\n",
    "                terms[i][term][0] += 1  # the times of term appears in i-document \n",
    "                if considered:  # term appears in which documents\n",
    "                    terms[i][term][1] += 1\n",
    "                    considered = False\n",
    "\n",
    "            if max_freq < terms[i][term][0]:\n",
    "                max_freq = terms[i][term][0]\n",
    "        max_freqs.append(max_freq)\n",
    "    return terms, max_freqs\n",
    "\n",
    "tf_df_matrix, max_freqs = tf_df_maxf(doc_tokenized)\n",
    "terms_ammount = len(distinct_terms)\n",
    "\n",
    "queries = [ word_tokenize('web development design') ]\n",
    "tf_df_in_query, max_freqs_in_query = tf_df_maxf(queries)\n",
    "\n",
    "# constructing terms matrix (i, j)\n",
    "def build_terms_matrix(len_x, len_y):\n",
    "    terms_matrix = np.zeros((len_x, len_y))\n",
    "    for i in range(number_docs):\n",
    "        doc = tf_df_matrix[i]\n",
    "        for t in doc:\n",
    "            t_index = distinct_terms[t]\n",
    "            terms_matrix[t_index][i] = doc[t][0]\n",
    "    return terms_matrix\n",
    "terms_matrix = build_terms_matrix(len(distinct_terms), number_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_df_matrix \n",
    "#[\n",
    "# {'word1': [f_11 df_1], 'word2': [f_21 df_2]}, -- doc 1\n",
    "# {'word1': [f_12 df_2], 'word2': [f_22 df_2]}, -- doc 2\n",
    "#]\n",
    "weight_tfidf = []\n",
    "distance_d_matrix = []\n",
    "for doc_index in range(len(tf_df_matrix)):\n",
    "    weight = []\n",
    "    doc_terms = tf_df_matrix[doc_index]\n",
    "    distance_d = 0\n",
    "    for key in doc_terms:\n",
    "        w = doc_terms[key][0]/max_freqs[doc_index] * math.log(number_docs/doc_terms[key][1], 2)\n",
    "        #w = terms_matrix[distinct_terms[key]][doc_index]/max_freqs[doc_index] * math.log(number_docs/doc_terms[key][1], 2)\n",
    "        weight.append(w)\n",
    "        distance_d += w ** 2\n",
    "    distance_d = math.sqrt(distance_d)\n",
    "    distance_d_matrix.append(distance_d)\n",
    "    weight_tfidf.append(weight)\n",
    "\n",
    "weight_query = []\n",
    "distance_q_matrix = []\n",
    "nominator_matrix = []\n",
    "for doc_index in range(len(tf_df_in_query)):\n",
    "    weight = []\n",
    "    doc_terms = tf_df_in_query[doc_index]\n",
    "    distance_q = 0\n",
    "    w_ij = 0\n",
    "    w_iq = 0\n",
    "    for key in doc_terms:\n",
    "        w_iq = 0.5 + 0.5 * doc_terms[key][0]/max_freqs_in_query[doc_index] * math.log(number_docs/doc_terms[key][1], 2)\n",
    "        weight.append(w_iq)\n",
    "        distance_q += w_iq ** 2\n",
    "        for i in range(len(tf_df_matrix)):\n",
    "            nominator = 0\n",
    "            if tf_df_matrix[i].get(key, -1) == -1:\n",
    "                pass\n",
    "            else:\n",
    "                w_ij = tf_df_matrix[i][key][0]/max_freqs[i] * math.log(number_docs/tf_df_matrix[i][key][1], 2)\n",
    "            nominator += w_ij * w_iq\n",
    "            nominator_matrix.append(nominator)\n",
    "\n",
    "    distance_q = math.sqrt(distance_q)\n",
    "    distance_q_matrix.append(distance_q)\n",
    "    weight_query.append(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriving documents \n",
    "- applying variables which store intermediate results for forms of these three models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Result:  Production design.txt  Regenerative design.txt  Glass art.txt  Graphic design.txt  Hardware interface design.txt\n",
      "\n",
      "Statistical Model Result:  Textile design.txt  Integrated circuit design.txt  Computer-aided design.txt  Nuclear weapon design.txt  Sustainable design.txt\n",
      "\n",
      "SVD Result:  Adobe Muse.txt  Signage.txt  Static web page.txt  Automotive suspension design.txt  Portal:Design.txt"
     ]
    }
   ],
   "source": [
    "# Getting all result of these three models\n",
    "def similarity(doc_index, query_index):\n",
    "    return nominator_matrix[doc_index]/(distance_q_matrix[query_index]*distance_d_matrix[doc_index])\n",
    "\n",
    "if debug:\n",
    "    relevant_res = {}\n",
    "    for i in range(number_docs):\n",
    "        relevant_res[similarity(i, 0)] = doc_names[i]\n",
    "\n",
    "    sorted_relevant_res = sorted(relevant_res.items(),  reverse=True)[:5]\n",
    "    print('TFIDF Result:', end=\"\")\n",
    "    for key in sorted_relevant_res:\n",
    "        print(' ', key[1], end=\"\")\n",
    "    print('\\n')\n",
    "    \n",
    "# Statistical Language Model\n",
    "# Pr(d_j | q) = Pr(q | d_j) * Pr(d_j)/Pr(q)\n",
    "def probability(doc_index, query_index):\n",
    "    doc = tf_df_matrix[doc_index]\n",
    "    query = tf_df_in_query[query_index]\n",
    "    pr_ti_dj = 1.0\n",
    "    for key in doc:\n",
    "        if query.get(key, -1) == -1:\n",
    "            pass\n",
    "        else:\n",
    "            pr_ti_dj *= math.pow(doc[key][0]/doc_lens[doc_index], query[key][0])\n",
    "\n",
    "    return pr_ti_dj\n",
    "if debug:\n",
    "    relevant_res = {}\n",
    "    for i in range(number_docs):\n",
    "        relevant_res[probability(i, 0)] = doc_names[i]\n",
    "\n",
    "    sorted_relevant_res = sorted(relevant_res.items(),  reverse=True)[:5]\n",
    "    print('Statistical Model Result:', end=\"\")\n",
    "    for key in sorted_relevant_res:\n",
    "        print(' ', key[1], end=\"\")\n",
    "    print('\\n')\n",
    "    \n",
    "# SVD\n",
    "def svd_relevant(term_matrix):\n",
    "    u, d, vh = np.linalg.svd(term_matrix)\n",
    "    u3 = u[:, :3]\n",
    "    d3 = np.diag(d[:3])\n",
    "    vh3 = np.transpose(vh[:,:3])\n",
    "    A3 = np.dot(np.dot(u3, d3), vh3)\n",
    "    # qk = qT*uk*inverse(dk)\n",
    "    qk_concept_space = []\n",
    "    for q in queries:\n",
    "        q_matrix = np.zeros((terms_ammount, 1))\n",
    "        for w in q:\n",
    "            index = distinct_terms.get(w, -1)\n",
    "            if index == -1:\n",
    "                pass\n",
    "            else:\n",
    "                q_matrix[index] = 1\n",
    "\n",
    "        qk_concept_space.append(np.dot(np.dot(np.transpose(q_matrix), u3), np.linalg.inv(d3)))\n",
    "    \n",
    "    #similarity qk and vk(documents)\n",
    "    relevant_res = {}\n",
    "    for i in range(number_docs):\n",
    "        for qk in qk_concept_space:\n",
    "            distance = np.dot(qk, vh3[:,i])/(np.linalg.norm(qk) * np.linalg.norm(vh3[:,i]))\n",
    "            relevant_res[distance[0]] = doc_names[i]\n",
    "    sorted_relevant_res = sorted(relevant_res.items(),  reverse=True)[:5]\n",
    "    print('SVD Result:', end=\"\")\n",
    "    for key in sorted_relevant_res:\n",
    "        print(' ', key[1], end=\"\")\n",
    "\n",
    "svd_relevant(terms_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
