{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 2\n",
    "\n",
    "You should work on the assignement in groups of 2 participants. \n",
    "\n",
    "Upload your solution as a jupyter notebook to L2P by 26th of June 23:59h. (The deadline is strict)\n",
    "\n",
    "Do not forget to specify the names of all contributing students in the jupyter notebook.\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dynamic PageRank\n",
    "Consider a random walk setting where the transistion matrix changes over time. At any point of time the probability of a random surfer to jump to a linked page is proportional to the number of previous visits. To start with all the pages are equally likely to be chosen but as the walk continues and the nodes are visited the transition probability changes as proportional to number of previous visits. For example let a page 'a' is linked to pages 'b', 'c' and 'd'. The random surfer currently resides at 'a' and the pages 'b', 'c' and 'd' have already been visited 5, 3 and 2 times respectively. The transition probability would be 0.5, 0.3 and 0.2 respectively. As a new node is viited the probabilities change. The random surfer continues to surf with probability 0.8. Generate 100 random walks and rank the nodes based on the frequency of visit. The random walk should be performed on a drected Erdos-Renyi graph with number of nodes n=200 and probability of edge creation p = 0.4. \n",
    "\n",
    "Hint: Use networkx library for generating graph.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "n = 200\n",
    "p = 0.4\n",
    "G = nx.erdos_renyi_graph(n, p, directed = True)\n",
    "nodes = list(G.nodes())\n",
    "visit_freq = {}\n",
    "for node in nodes:\n",
    "    visit_freq[node] = 0\n",
    "\n",
    "p_restart = 1 - 0.8\n",
    "random.shuffle(nodes)\n",
    "walks = 100\n",
    "for i in range(walks):\n",
    "    random.shuffle(nodes)\n",
    "    current_node = nodes[0]\n",
    "    visit_freq[current_node] += 1\n",
    "    \n",
    "    while random.uniform(0, 1) > p_restart: # with 0.8 probability continue to do this walk\n",
    "        neighbors = list(G.neighbors(current_node))\n",
    "        for neighbor in neighbors:\n",
    "            random.shuffle(neighbors) # randomly choosing a neighbor\n",
    "            current_node = neighbors[0]\n",
    "            visit_freq[current_node] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recommendation\n",
    "a. Compare the recommendation algorithms (SVD, NMF, Baseline, k-NN and Random) available in surprise package on movielens dataset in terms of RMSE and MAE.\n",
    "\n",
    "b. Consider the movielens dataset and divide it into (i) training set with 50% of the data (train the algorithms on this part) and (ii) 25% validation set and (iii) test set with the rest. Estimate the ratings of the test set using the algorithms (same as in a) provided by the package on the training set. Your final rating should be weighted average of the ratings predicted by the algorithms. The weights should be learnt on the validation set. Performance should be measured in terms of RMSE.\n",
    "\n",
    "Hint: Use grid search/step-wise update like SGD for learning the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD: 0.945 0.747\n",
      "NMF: 0.975 0.766\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Baseline: 0.948 0.752\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "k-NN: 0.987 0.780\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Random: 1.519 1.219\n",
      "0.6672805089811458\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD, NMF, BaselineOnly, KNNBasic, NormalPredictor\n",
    "from surprise import Dataset, accuracy\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.model_selection import train_test_split \n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "# x: input examples, y: y value for each x, w: weights matrix\n",
    "# l_rate: learning rate, m: number of input examples\n",
    "# epochs: the number of iterations\n",
    "def gradient_descent(x, y, w, l_rate, m, epochs):\n",
    "    x_trans = x.transpose()\n",
    "    for i in range(0, epochs):\n",
    "        hypothesis = np.dot(x, w)\n",
    "        loss = hypothesis - y\n",
    "        # avg cost\n",
    "        cost = np.sum(loss ** 2) / (2 * m)\n",
    "        #print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        # avg gradient\n",
    "        gradient = np.dot(x_trans, loss) / m\n",
    "        # update weight\n",
    "        w = w - l_rate * gradient\n",
    "    return w\n",
    "\n",
    "def final_predicted_rating(x, w):\n",
    "    hypothesis = np.dot(x, w)\n",
    "    return hypothesis\n",
    "\n",
    "def build_examples(dataset):    \n",
    "    predictions = []\n",
    "    # stroing prediction of each model respectively.\n",
    "    for model in algo_names:\n",
    "        prediction = models[model].test(dataset)\n",
    "        p_array = []\n",
    "        for id, p in enumerate(prediction):\n",
    "            # user: 311     item: 15         r_ui = 5.00   est = 4.07\n",
    "            # p[0] -- user, p[1] --- item\n",
    "            p_array.append((p[0], p[1], p.est))\n",
    "        predictions.append(p_array)\n",
    "\n",
    "    # predicted values of each algorithm\n",
    "    est_values = []\n",
    "    for idx in range(len(predictions[0])):\n",
    "        values = []\n",
    "        for idy in range(len(predictions)):\n",
    "            # predictions[idy][idx][2]: predicted value of algorithm with index \"idx\" for one item.\n",
    "            values.append(predictions[idy][idx][2])\n",
    "        est_values.append(values)\n",
    "    est_values = np.array(est_values)\n",
    "\n",
    "    # all original ratings\n",
    "    rating_values = []\n",
    "    for v in dataset:\n",
    "        rating_values.append(v[2])\n",
    "    rating_values = np.array(rating_values)\n",
    "    return est_values, rating_values\n",
    "\n",
    "\n",
    "algorithms = [SVD, NMF, BaselineOnly, KNNBasic, NormalPredictor]\n",
    "\n",
    "# Load the movielens-100k dataset (download it if needed).\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset, restset = train_test_split(data, test_size=0.5) # 50% training set, 50% for validation and test\n",
    "validationset = restset[:round(len(restset)/2)]\n",
    "testset = restset[round(len(restset)/2):]\n",
    "# user film rating ('506', '731', 4.0)\n",
    "\n",
    "algo_names = ['SVD', 'NMF', 'Baseline', 'k-NN', 'Random']\n",
    "models = {}\n",
    "rmse_mae = []\n",
    "accuracies = {}\n",
    "# Algorithms.\n",
    "for index, algo in enumerate(algorithms):\n",
    "    # for subtask a), Run 5-fold cross-validation and print results. \n",
    "    out = cross_validate(algo(), data, measures=['RMSE', 'MAE'], cv=3)\n",
    "    #rmse_mae.append( ['{:.3f}'.format(np.mean(out['test_rmse'])), '{:.3f}'.format(np.mean(out['test_mae']))] )\n",
    "    print(algo_names[index] + ':', 'RMSE {:.3f}'.format(np.mean(out['test_rmse'])), 'MAE {:.3f}'.format(np.mean(out['test_mae'])))\n",
    "    \n",
    "    # subtask b)\n",
    "    algo = algo()\n",
    "    algo.fit(trainset)\n",
    "    models[algo_names[index]] = algo\n",
    "\n",
    "\n",
    "v_est_values, v_rating_values = build_examples(validationset)\n",
    "t_est_values, t_rating_values = build_examples(testset)\n",
    "# weights\n",
    "weights = np.ones(len(algorithms))\n",
    "learning_rate = 0.0005\n",
    "epochs = 10000\n",
    "v_m, v_n = np.shape(est_values)\n",
    "t_m, t_n = np.shape(t_est_values)\n",
    "# learning weight on validation set\n",
    "weights = gradient_descent(v_est_values, v_rating_values, weights, learning_rate, v_m, epochs)\n",
    "print('weights:', weights)\n",
    "\n",
    "# final ratings\n",
    "final_ratings = final_predicted_rating(t_est_values, weights)\n",
    "\n",
    "# performance\n",
    "final_rmse = np.sqrt(np.sum((final_ratings - t_rating_values) ** 2) / (2 * t_m))\n",
    "print('Perfomance of combiniation of five algorithms, final RMSE:'final_rmse)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hidden Markov model\n",
    "Consider the HMM package https://hmmlearn.readthedocs.io/en/latest/\n",
    "\n",
    "a. Generate sequences with multinomial HMM (2 symbols and 4 hidden states) and given parameters. Start probability - {0.4,0.2,0.1,0.3}, Transition matrix - {{0.2,0.3,0.1,0.4},{0.3,0.3,0.2,0.2},{0.4,0.2,0.3,0.1},{0.2,0.3,0.1,0.4}}, Emission probability - {{0.2,0.8},{0.1,0.9},{0.5,0.5},{0.6,0.4}}.\n",
    "\n",
    "\n",
    "b. Consider a sequence - {1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1}. Fit a multinomial HMM considering 4 states and obtain hidden state which is most likely to have generated the symbol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialHMM(algorithm='viterbi', init_params='ste', n_components=4,\n",
       "        n_iter=10, params='ste',\n",
       "        random_state=<mtrand.RandomState object at 0x1092f74c8>,\n",
       "        startprob_prior=1.0, tol=0.01, transmat_prior=1.0, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn.hmm import MultinomialHMM\n",
    "\n",
    "startprob = np.array([0.4,0.2,0.1,0.3])\n",
    "transition_matrix = np.array([[0.2,0.3,0.1,0.4],\n",
    "                             [0.3,0.3,0.2,0.2],\n",
    "                             [0.3,0.3,0.2,0.2],\n",
    "                             [0.4,0.2,0.3,0.1],\n",
    "                             [0.2,0.3,0.1,0.4]])\n",
    "emissionprob = np.array([[0.2,0.8],\n",
    "                        [0.1,0.9],\n",
    "                        [0.5,0.5],\n",
    "                        [0.6,0.4]])\n",
    "seq = [[1], [0], [0], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [1]]\n",
    "\n",
    "model = MultinomialHMM(n_components=4)\n",
    "model.transmat_ = transition_matrix\n",
    "model.startprob_ = startprob\n",
    "model.emissionprob_ = emissionprob\n",
    "X, Z = model.sample(100) # a\n",
    "model.fit(seq) # b\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PrefixSpan \n",
    "Implement the Prefix algorithm for Sequential Pattern Mining.\n",
    "\n",
    "Input of the algorithm :  \n",
    "\n",
    "\tA sequence database S, \n",
    "and the minimum support threshold min_support.\n",
    "\n",
    " - Output of the algorithm:  \n",
    "\t The complete set of sequential patterns.\n",
    " - Subroutine: PrefixSpan(α, L, S|α).\n",
    " \n",
    "Parameters: \n",
    "- α: sequential pattern, \n",
    "- L: the length of α; \n",
    "- S|α: : the α-projected database, if α ≠<>; otherwise;  the sequence database S.\n",
    "\n",
    "Call PrefixSpan(<>,0,S).\n",
    "\n",
    "1. Scan S|α once, \n",
    "find the set of frequent items b such that:\n",
    " \n",
    " b can be assembled to the last element of α to form a sequential pattern; or\n",
    " \\<b\\> can be appended to α to form a sequential pattern.\n",
    " \n",
    "2. For each frequent item b:\n",
    "- append it to α to form a sequential pattern α’ and output α’;\n",
    "- output α’;\n",
    "\n",
    "3. For each α’:\n",
    " - construct α’-projected database S|α’ and call PrefixSpan(α’, L+1, S|α’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "[[1]]\n",
      "[[1, 2]]\n",
      "[[1, 2], [3]]\n",
      "[[2]]\n",
      "[[3]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# [a[b,c], [bcd[ab]]] three-dimensional\n",
    "class PrefixSpan:\n",
    "    \n",
    "    def __init__(self, sequences, min_support=0.5):\n",
    "        \n",
    "        self.min_support = round(min_support * len(sequences))\n",
    "        self.PLACE_HOLDER = '_'\n",
    "        self.s_patterns = None\n",
    "        self.s_patterns = self.prefix_span(self.SequencePattern([], None, self.PLACE_HOLDER), \n",
    "                                           sequences, 0, self.min_support, None)\n",
    "        \n",
    "    def get_all_patterns(self):\n",
    "        return self.s_patterns\n",
    "        \n",
    "    def prefix_span(self, seq_pattern, sequences, pattern_length, support, test):\n",
    "        patterns = list()\n",
    " \n",
    "        if len(sequences) < support:\n",
    "            return patterns\n",
    "        \n",
    "        f_items = self.frequent_item(sequences, seq_pattern, support)\n",
    "        \n",
    "        for f_i in f_items:\n",
    "            #print('current frequent item:', f_i.sequence, 'pattern seq:', id(seq_pattern.sequence))\n",
    "            p = self.SequencePattern(seq_pattern.sequence, seq_pattern.freq, self.PLACE_HOLDER)\n",
    "            \n",
    "            \n",
    "            p.append(f_i)\n",
    "            patterns.append(p)\n",
    "            tmp = copy.deepcopy(p) # workaround\n",
    "            p_db = self.build_projected_database(sequences, p)\n",
    "            p_patterns = self.prefix_span(tmp, p_db, 0, support, patterns)\n",
    "            patterns.extend(p_patterns)\n",
    "            \n",
    "        return patterns\n",
    "        \n",
    "    \n",
    "    def frequent_item(self, seqs, pattern, support):\n",
    "        items = {} # normal patterns <b> + <c> => <bc>\n",
    "        multi_items = {} # special patterns <(ab> + <c)> => (<abc)>\n",
    "        f_item_list = []\n",
    "        \n",
    "        if seqs is None or len(seqs) == 0:\n",
    "            return []\n",
    "        \n",
    "        if len(pattern.sequence) != 0:\n",
    "            last_el = pattern.sequence[-1]\n",
    "        else:\n",
    "            last_el = []\n",
    "            \n",
    "        for seq in seqs:\n",
    "            # 1. Pattern\n",
    "            # current pattern: [[...], [a,b]], database: [[a,b,c],[...]]\n",
    "            # attaching item c to the element [a,b] of the pattern, getting the new pattern [a,b,c]\n",
    "            if self.PLACE_HOLDER != seq[0][0]:\n",
    "                is_prefix = True\n",
    "                for item in last_el: #test last element of the given pattern occurs \n",
    "                    if item not in seq[0]:\n",
    "                        is_prefix = False\n",
    "                        break\n",
    "                if is_prefix and len(last_el) > 0:\n",
    "                    last_item = last_el[-1]\n",
    "                    item_index = seq[0].index(last_item)\n",
    "                    if item_index < len(seq[0]):\n",
    "                        for item in seq[0][item_index+1:]:\n",
    "                            if item not in multi_items:\n",
    "                                multi_items[item] = 1\n",
    "                            else:\n",
    "                                multi_items[item] += 1\n",
    "                                \n",
    "            # 2. Pattern\n",
    "            # current pattern: [[...], [a,b]], database: [[_,c],[...]]\n",
    "            # attaching item c to the element [a,b] of the pattern, getting the new pattern [a,b,c]\n",
    "            elif self.PLACE_HOLDER == seq[0][0]:\n",
    "                for item in seq[0][1:]:\n",
    "                    if item in multi_items:\n",
    "                        multi_items[item] += 1\n",
    "                    else:\n",
    "                        multi_items[item] = 1\n",
    "                \n",
    "                seq = seq[1:] # skipping the first element seq[0], because we have collected all pattern from seq[0]\n",
    "            # 3. Pattern\n",
    "            recorded = []\n",
    "            for el in seq:\n",
    "                for item in el:\n",
    "                    if item not in recorded:\n",
    "                        recorded.append(item)\n",
    "                        if item not in items:\n",
    "                            items[item] = 1\n",
    "                        else:\n",
    "                            items[item] += 1\n",
    "            \n",
    "        # applying each character in multi_items and items having minimal support to build up all new patterns\n",
    "        f_item_list.extend([self.SequencePattern([[self.PLACE_HOLDER, key]], value, self.PLACE_HOLDER) \n",
    "                          for key, value in multi_items.items() if value >= support])\n",
    "        f_item_list.extend([self.SequencePattern([[key]], value, self.PLACE_HOLDER) \n",
    "                          for key, value in items.items() if value >= support])\n",
    "        #print('multi_items:', multi_items)\n",
    "#         for f in f_item_list:\n",
    "#             print('f_item_list:', f.sequence)\n",
    "        return f_item_list\n",
    "            \n",
    "            \n",
    "    class SequencePattern:\n",
    "        def __init__(self, item_set, item_freq, place_holder):\n",
    "            self.sequence = []\n",
    "            self.place_holder = place_holder\n",
    "            self.freq = None # applying for sorting\n",
    "            for item in item_set:\n",
    "                self.sequence.append(item)\n",
    "            self.freq = item_freq\n",
    "        \n",
    "        def append(self, pattern):\n",
    "            if pattern.sequence[0][0] == self.place_holder:\n",
    "                self.sequence[-1].extend(pattern.sequence[0][1:])\n",
    "                self.sequence.extend(pattern.sequence[1:])\n",
    "            else:\n",
    "                self.sequence.extend(pattern.sequence)\n",
    "                \n",
    "                if self.freq is None:\n",
    "                    self.freq = pattern.freq\n",
    "            self.freq = min(self.freq, pattern.freq)\n",
    "                 \n",
    "\n",
    "    def build_projected_database(self, seqs, pattern):\n",
    "        projected_seqs = []\n",
    "        \n",
    "        if pattern is None or seqs is None:\n",
    "            return []\n",
    "        \n",
    "        # pattern.sequence = [[1],[2],[3,4]] : 12(34)\n",
    "        last_el = pattern.sequence[-1]\n",
    "        last_item = last_el[-1]\n",
    "        for seq in seqs:\n",
    "            p_seqs = []\n",
    "            for el in seq:\n",
    "                is_prefix = False\n",
    "                if el[0] == self.PLACE_HOLDER: # el=[_ab] or [a_b]? the order within an element no matters\n",
    "                    if last_item in el and len(pattern.sequence[-1]) > 1: # since '_' and another char in el so that at least 2 chars\n",
    "                        is_prefix = True\n",
    "                else:\n",
    "                    is_prefix = True\n",
    "                    for item in last_el:\n",
    "                        if item not in el:\n",
    "                            is_prefix = False\n",
    "                            break\n",
    "                \n",
    "                if is_prefix:\n",
    "                    el_index = seq.index(el)\n",
    "                    item_index = el.index(last_item)\n",
    "                    if item_index == len(el) - 1:\n",
    "                        p_seqs = seq[el_index + 1:]\n",
    "                    else:\n",
    "                        p_seqs = seq[el_index:]\n",
    "                        tmp_el = el[item_index:]\n",
    "                        tmp_el[0] = self.PLACE_HOLDER\n",
    "                        p_seqs[0] = tmp_el\n",
    "                        \n",
    "                        \n",
    "                    break # find the corresponding projected database of given pattern in current sequence\n",
    "                \n",
    "            if len(p_seqs) != 0:\n",
    "                projected_seqs.append(p_seqs)\n",
    "        return projected_seqs\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sequences = [\n",
    "        [[1,2],[3]],\n",
    "        [[1],[3,2],[1,2]],\n",
    "        [[1,2],[5]],\n",
    "        [[6]],\n",
    "    ]\n",
    "\n",
    "    model = PrefixSpan(sequences, min_support=0.5)\n",
    "    result = model.get_all_patterns()\n",
    "    print('Result:')\n",
    "    for fs in result:\n",
    "        #print('{}, {}'.format(fs.sequence,fs.freq))\n",
    "        print('{}'.format(fs.sequence))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
